{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc6a4d4",
   "metadata": {},
   "source": [
    "# Engineering AI Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for engineering knowledge using an NVIDIA RTX 4060 GPU. The model will be trained to answer engineering questions like \"How is a car built?\" with detailed and accurate responses.\n",
    "\n",
    "## Prerequisites\n",
    "- NVIDIA RTX 4060 GPU\n",
    "- CUDA installed\n",
    "- Python 3.8+\n",
    "- Virtual environment `ai-notebooks-env` activated\n",
    "- Sufficient disk space (at least 10GB for models and data)\n",
    "\n",
    "## Setup Instructions\n",
    "Before running this notebook:\n",
    "1. Create virtual environment: `python -m venv ai-notebooks-env`\n",
    "2. Activate it: `ai-notebooks-env\\Scripts\\activate` (Windows)\n",
    "3. Install Jupyter: `pip install jupyter notebook`\n",
    "4. Start notebook: `jupyter notebook engineering_ai_model.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dbb9d",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary libraries for model training and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07c050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Check\n",
      "==================================================\n",
      "Python version: 3.9.13\n",
      "‚ö†Ô∏è  Python < 3.10 detected - will use compatible package versions\n",
      "   For best compatibility, consider upgrading to Python 3.10+\n",
      "‚úÖ Virtual environment active: ai-notebooks-env\n",
      "‚úÖ Correct virtual environment (ai-notebooks-env)\n",
      "Free disk space: 721.7 GB\n",
      "‚úÖ Sufficient disk space\n",
      "‚úÖ NVIDIA GPU detected\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check virtual environment and system prerequisites\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç System Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check Python version\n",
    "python_version = sys.version_info\n",
    "print(f\"Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "# Check if Python version is compatible\n",
    "if python_version < (3, 8):\n",
    "    print(\"‚ùå Python 3.8+ required!\")\n",
    "    print(\"   Please upgrade your Python version\")\n",
    "elif python_version < (3, 10):\n",
    "    print(\"‚ö†Ô∏è  Python < 3.10 detected - will use compatible package versions\")\n",
    "    print(\"   For best compatibility, consider upgrading to Python 3.10+\")\n",
    "else:\n",
    "    print(\"‚úÖ Compatible Python version\")\n",
    "\n",
    "# Check if we're in virtual environment\n",
    "venv_path = os.environ.get('VIRTUAL_ENV')\n",
    "if venv_path:\n",
    "    print(f\"‚úÖ Virtual environment active: {os.path.basename(venv_path)}\")\n",
    "    if 'ai-notebooks-env' in venv_path:\n",
    "        print(\"‚úÖ Correct virtual environment (ai-notebooks-env)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Different virtual environment detected\")\n",
    "        print(f\"   Expected: ai-notebooks-env\")\n",
    "        print(f\"   Current: {os.path.basename(venv_path)}\")\n",
    "else:\n",
    "    print(\"‚ùå No virtual environment detected!\")\n",
    "    print(\"   Please activate ai-notebooks-env:\")\n",
    "    print(\"   ai-notebooks-env\\\\Scripts\\\\activate\")\n",
    "\n",
    "# Check disk space\n",
    "try:\n",
    "    import shutil\n",
    "    free_gb = shutil.disk_usage('.')[2] / (1024**3)\n",
    "    print(f\"Free disk space: {free_gb:.1f} GB\")\n",
    "    if free_gb < 15:\n",
    "        print(\"‚ö†Ô∏è  Low disk space! Recommend at least 15GB free\")\n",
    "    else:\n",
    "        print(\"‚úÖ Sufficient disk space\")\n",
    "except:\n",
    "    print(\"Could not check disk space\")\n",
    "\n",
    "# Check CUDA availability (preliminary check)\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, shell=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ NVIDIA GPU detected\")\n",
    "    else:\n",
    "        print(\"‚ùå NVIDIA GPU not detected or drivers not installed\")\n",
    "except:\n",
    "    print(\"‚ùå Could not check GPU status\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a7bca",
   "metadata": {},
   "source": [
    "### Virtual Environment Setup Instructions\n",
    "\n",
    "If you haven't set up the virtual environment yet, run these commands in PowerShell/CMD:\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv ai-notebooks-env\n",
    "\n",
    "# Activate virtual environment (Windows)\n",
    "ai-notebooks-env\\Scripts\\activate\n",
    "\n",
    "# Install Jupyter in the virtual environment\n",
    "pip install jupyter notebook ipykernel\n",
    "\n",
    "# Add kernel to Jupyter\n",
    "python -m ipykernel install --user --name ai-notebooks-env --display-name \"AI Notebooks (ai-notebooks-env)\"\n",
    "\n",
    "# Start Jupyter Notebook\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Important**: Make sure you select the \"AI Notebooks (ai-notebooks-env)\" kernel when running this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing required packages...\n",
      "This may take several minutes for the first run.\n",
      "============================================================\n",
      "‚ö†Ô∏è  Python 3.9 detected\n",
      "   Installing compatible package versions...\n",
      "Requirement already satisfied: pip in c:\\users\\tivanov\\source\\ai-notebooks\\ai-notebooks-env\\lib\\site-packages (25.2)\n",
      "Installing PyTorch with CUDA 12.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\tivanov\\source\\ai-notebooks\\ai-notebooks-env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\tivanov\\source\\ai-notebooks\\ai-notebooks-env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\tivanov\\source\\ai-notebooks\\ai-notebooks-env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages in virtual environment with compatibility fixes\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "print(\"This may take several minutes for the first run.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check Python version for compatibility\n",
    "python_version = sys.version_info\n",
    "is_old_python = python_version < (3, 10)\n",
    "\n",
    "if is_old_python:\n",
    "    print(f\"‚ö†Ô∏è  Python {python_version.major}.{python_version.minor} detected\")\n",
    "    print(\"   Installing compatible package versions...\")\n",
    "\n",
    "# Upgrade pip first\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "print(\"Installing PyTorch with CUDA 12.1...\")\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install transformers and related libraries with version constraints for older Python\n",
    "print(\"Installing Transformers and ML libraries...\")\n",
    "if is_old_python:\n",
    "    print(\"   Using Python < 3.10 compatible versions...\")\n",
    "    !pip install \"transformers<4.36.0\" \"datasets<2.15.0\" \"accelerate<0.25.0\"\n",
    "else:\n",
    "    !pip install transformers datasets accelerate\n",
    "\n",
    "# Install additional packages\n",
    "print(\"Installing additional libraries...\")\n",
    "!pip install scikit-learn matplotlib seaborn\n",
    "\n",
    "# Install optional packages (with error handling)\n",
    "print(\"Installing optional packages...\")\n",
    "try:\n",
    "    !pip install wandb --quiet\n",
    "    print(\"‚úÖ wandb installed (for experiment tracking)\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  wandb installation failed (optional)\")\n",
    "\n",
    "# Verify installations\n",
    "print(\"\\nüîç Verifying installations...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} installed\")\n",
    "    \n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers {transformers.__version__} installed\")\n",
    "    \n",
    "    import datasets\n",
    "    print(f\"‚úÖ Datasets library installed\")\n",
    "    \n",
    "    import sklearn\n",
    "    print(f\"‚úÖ Scikit-learn installed\")\n",
    "    \n",
    "    print(\"‚úÖ All core packages installed successfully!\")\n",
    "    \n",
    "    if is_old_python:\n",
    "        print(\"\\nüí° Note: Using compatibility versions for Python < 3.10\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation error: {e}\")\n",
    "    print(\"Please check your virtual environment and try again.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009891dc",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies and Check GPU\n",
    "\n",
    "Let's import the necessary libraries and verify that our NVIDIA RTX 4060 is available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14225a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d4ba9",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "We'll create a dataset of engineering questions and answers to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineering Q&A dataset\n",
    "engineering_qa_data = [\n",
    "    {\n",
    "        \"question\": \"How is a car built?\",\n",
    "        \"answer\": \"Car manufacturing involves several key stages: 1) Design and Engineering - CAD modeling, prototyping, and testing. 2) Body Manufacturing - stamping steel sheets into body panels, welding them together in the body shop. 3) Paint Shop - cleaning, priming, painting, and clear coating the body. 4) Assembly Line - installing the engine, transmission, electrical systems, interior components, wheels, and final quality checks. The process uses lean manufacturing principles, robotics for precision tasks, and just-in-time inventory management.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the main components of an internal combustion engine?\",\n",
    "        \"answer\": \"The main components include: 1) Engine Block - houses cylinders and crankshaft. 2) Pistons - move up and down in cylinders to compress fuel-air mixture. 3) Connecting Rods - link pistons to crankshaft. 4) Crankshaft - converts linear piston motion to rotational motion. 5) Valves - control intake of fuel-air mixture and exhaust of combustion gases. 6) Camshaft - operates the valves via cams. 7) Spark Plugs - ignite the compressed fuel-air mixture. 8) Fuel System - delivers fuel to cylinders. 9) Cooling System - prevents overheating.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does a bridge support heavy loads?\",\n",
    "        \"answer\": \"Bridges support loads through structural engineering principles: 1) Load Distribution - spreading weight across multiple support points. 2) Material Strength - using high-strength materials like steel and reinforced concrete. 3) Structural Design - beam bridges use compression and tension, arch bridges use compression, suspension bridges use tension cables. 4) Foundation Systems - deep foundations transfer loads to stable soil or bedrock. 5) Safety Factors - designing for loads much greater than expected maximum. 6) Regular Inspection - monitoring for fatigue, corrosion, and structural integrity.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the engineering process for designing a building?\",\n",
    "        \"answer\": \"Building design follows a systematic process: 1) Programming - understanding client needs, site analysis, and code requirements. 2) Schematic Design - initial concepts, massing studies, and basic layouts. 3) Design Development - detailed systems integration, structural, mechanical, electrical design. 4) Construction Documents - detailed drawings and specifications for construction. 5) Permitting - obtaining necessary approvals from authorities. 6) Construction Administration - overseeing construction, reviewing submittals, and ensuring quality. Each phase involves coordination between architects, structural, mechanical, electrical, and civil engineers.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do airplane wings generate lift?\",\n",
    "        \"answer\": \"Airplane wings generate lift through aerodynamic principles: 1) Bernoulli's Principle - curved wing shape causes air to move faster over the top surface, creating lower pressure above the wing. 2) Newton's Third Law - wing deflects air downward, creating an equal and opposite upward force (lift). 3) Angle of Attack - tilting the wing increases lift up to the stall angle. 4) Airfoil Shape - specially designed cross-section optimizes lift-to-drag ratio. 5) Wing Design Features - flaps, slats, and winglets enhance performance. The amount of lift depends on air density, wing area, airspeed, and angle of attack.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the key principles of mechanical engineering?\",\n",
    "        \"answer\": \"Mechanical engineering is built on fundamental principles: 1) Mechanics - statics (forces in equilibrium) and dynamics (motion and forces). 2) Thermodynamics - energy conversion, heat transfer, and fluid mechanics. 3) Materials Science - understanding material properties, strength, fatigue, and failure modes. 4) Design Process - problem identification, concept generation, analysis, optimization, and testing. 5) Manufacturing - processes like machining, casting, welding, and assembly. 6) Control Systems - feedback loops, sensors, actuators for automated systems. 7) Safety and Reliability - factor of safety, failure analysis, and quality control.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does a hydraulic system work?\",\n",
    "        \"answer\": \"Hydraulic systems work on Pascal's principle: 1) Incompressible Fluid - oil or hydraulic fluid transmits pressure equally in all directions. 2) Pressure Multiplication - small force on small piston creates large force on large piston. 3) System Components - reservoir, pump, filters, valves, actuators (cylinders or motors), and connecting lines. 4) Control Valves - directional, pressure, and flow control valves manage system operation. 5) Applications - construction equipment, aircraft controls, automotive brakes, and industrial machinery. Advantages include high power-to-weight ratio, precise control, and ability to hold loads without continuous power input.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the engineering behind electric motors?\",\n",
    "        \"answer\": \"Electric motors convert electrical energy to mechanical energy: 1) Electromagnetic Induction - current-carrying conductors in magnetic fields experience force. 2) Motor Types - DC motors (brushed/brushless), AC induction motors, synchronous motors, stepper motors. 3) Key Components - stator (stationary part with electromagnets), rotor (rotating part), commutator (in DC motors), bearings. 4) Control Systems - variable frequency drives for AC motors, electronic speed controllers for DC motors. 5) Efficiency Factors - motor design, materials, manufacturing precision, and operating conditions. Applications range from tiny servo motors to large industrial drives.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(engineering_qa_data)\n",
    "print(f\"Dataset size: {len(df)} question-answer pairs\")\n",
    "print(\"\\nSample questions:\")\n",
    "for i, question in enumerate(df['question'][:3]):\n",
    "    print(f\"{i+1}. {question}\")\n",
    "\n",
    "# Create training format (instruction-following format)\n",
    "def format_training_data(question, answer):\n",
    "    return f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "df['text'] = df.apply(lambda row: format_training_data(row['question'], row['answer']), axis=1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTrain size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "\n",
    "# Display sample formatted training data\n",
    "print(\"\\nSample formatted training data:\")\n",
    "print(train_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8818853",
   "metadata": {},
   "source": [
    "## 4. Load and Configure Base Model\n",
    "\n",
    "We'll use GPT-2 as our base model and configure it for fine-tuning on engineering knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37839b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # Using GPT-2 as base model (good for RTX 4060)\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"Question: How is a car built?\\nAnswer:\"\n",
    "tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "print(f\"\\nSample tokenization:\")\n",
    "print(f\"Input text: {sample_text}\")\n",
    "print(f\"Token IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Tokens: {tokens['input_ids'][0][:10]}...\")  # Show first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a62273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512,  # Reasonable length for RTX 4060\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # For causal language modeling, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text']])\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Sample tokenized data keys: {train_dataset[0].keys()}\")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b20dab",
   "metadata": {},
   "source": [
    "## 5. Set Up Training Configuration\n",
    "\n",
    "Configure training parameters optimized for NVIDIA RTX 4060 (8GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ad026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for RTX 4060\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./engineering-model-checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Small batch size for 8GB VRAM\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    warmup_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Use mixed precision to save memory\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=None,  # Disable wandb for now\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Mixed precision (fp16): {training_args.fp16}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f412e4",
   "metadata": {},
   "source": [
    "## 6. Fine-tune the Model\n",
    "\n",
    "Now we'll train the model on our engineering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d57bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"\\nTraining results:\")\n",
    "    print(f\"- Final training loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"- Training steps: {train_result.global_step}\")\n",
    "    print(f\"- Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(\"./fine-tuned-engineering-model\")\n",
    "    tokenizer.save_pretrained(\"./fine-tuned-engineering-model\")\n",
    "    print(\"\\nModel saved to './fine-tuned-engineering-model'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    print(\"This might be due to memory constraints. Try reducing batch size or sequence length.\")\n",
    "\n",
    "print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14404e55",
   "metadata": {},
   "source": [
    "## 7. Test Model with Engineering Questions\n",
    "\n",
    "Let's test our trained model with various engineering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b12437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers\n",
    "def ask_engineering_question(question, max_length=200):\n",
    "    \"\"\"Generate an answer to an engineering question\"\"\"\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=len(inputs.input_ids[0]) + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"Answer:\" in response:\n",
    "        answer = response.split(\"Answer:\", 1)[1].strip()\n",
    "        return answer\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How is a car built?\",\n",
    "    \"What are the main components of a bridge?\",\n",
    "    \"How does a jet engine work?\",\n",
    "    \"What is the engineering process for designing a skyscraper?\",\n",
    "    \"How do solar panels convert sunlight to electricity?\",\n",
    "    \"What are the key principles of structural engineering?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        answer = ask_engineering_question(question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "    \n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc4366",
   "metadata": {},
   "source": [
    "## 8. Compare with Base Model\n",
    "\n",
    "Let's compare our fine-tuned model against the original GPT-2 model to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91525a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original GPT-2 for comparison\n",
    "print(\"Loading original GPT-2 for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "def ask_base_model(question, max_length=200):\n",
    "    \"\"\"Generate answer using base GPT-2 model\"\"\"\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=len(inputs.input_ids[0]) + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=base_tokenizer.eos_token_id,\n",
    "            eos_token_id=base_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in response:\n",
    "        answer = response.split(\"Answer:\", 1)[1].strip()\n",
    "        return answer\n",
    "    return response\n",
    "\n",
    "# Compare models on the main question\n",
    "comparison_question = \"How is a car built?\"\n",
    "\n",
    "print(f\"Comparison Question: {comparison_question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nü§ñ BASE GPT-2 RESPONSE:\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    base_answer = ask_base_model(comparison_question)\n",
    "    print(base_answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\nüîß FINE-TUNED MODEL RESPONSE:\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    finetuned_answer = ask_engineering_question(comparison_question)\n",
    "    print(finetuned_answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Notice how the fine-tuned model provides more detailed and\")\n",
    "print(\"engineering-specific responses compared to the base model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d6d33",
   "metadata": {},
   "source": [
    "## 9. Save and Load Trained Model\n",
    "\n",
    "Instructions for saving and reloading your trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (if not already saved)\n",
    "model_save_path = \"./fine-tuned-engineering-model\"\n",
    "\n",
    "print(\"Saving trained model...\")\n",
    "try:\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"‚úÖ Model saved successfully to: {model_save_path}\")\n",
    "    \n",
    "    # List saved files\n",
    "    import os\n",
    "    saved_files = os.listdir(model_save_path)\n",
    "    print(f\"Saved files: {saved_files}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"To load this model in the future, use:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained('./fine-tuned-engineering-model')\n",
    "model = AutoModelForCausalLM.from_pretrained('./fine-tuned-engineering-model')\n",
    "\n",
    "# Use the model for inference\n",
    "def ask_question(question):\n",
    "    prompt = f\"Question: {question}\\\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=200, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"Answer:\", 1)[1].strip()\n",
    "\n",
    "# Example usage\n",
    "answer = ask_question(\"How is a car built?\")\n",
    "print(answer)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da4700",
   "metadata": {},
   "source": [
    "## 10. Interactive Q&A Interface\n",
    "\n",
    "Create a simple interactive interface to test your model with custom questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A function\n",
    "def interactive_qa():\n",
    "    \"\"\"Interactive function to ask engineering questions\"\"\"\n",
    "    print(\"üîß Engineering AI Assistant\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Ask me any engineering question! (type 'quit' to exit)\")\n",
    "    print(\"Examples:\")\n",
    "    print(\"- How is a car built?\")\n",
    "    print(\"- What are the main components of a bridge?\")\n",
    "    print(\"- How does a jet engine work?\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\n‚ùì Your question: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'stop']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            if not question:\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nü§î Thinking...\")\n",
    "            answer = ask_engineering_question(question, max_length=150)\n",
    "            print(f\"\\nüîß Answer: {answer}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Uncomment the line below to run the interactive interface\n",
    "# interactive_qa()\n",
    "\n",
    "# Alternative: Test with a few predefined questions\n",
    "print(\"üîß Engineering AI Assistant - Demo Mode\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo_questions = [\n",
    "    \"How does a wind turbine generate electricity?\",\n",
    "    \"What materials are used in aircraft construction?\",\n",
    "    \"How is concrete made stronger?\",\n",
    "]\n",
    "\n",
    "for question in demo_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        answer = ask_engineering_question(question, max_length=120)\n",
    "        print(f\"üîß Answer: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240fbd4",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What we accomplished:\n",
    "1. ‚úÖ Set up the environment for training on NVIDIA RTX 4060\n",
    "2. ‚úÖ Created an engineering-focused question-answer dataset\n",
    "3. ‚úÖ Fine-tuned GPT-2 on engineering knowledge\n",
    "4. ‚úÖ Tested the model with various engineering questions\n",
    "5. ‚úÖ Compared performance against the base model\n",
    "6. ‚úÖ Saved the trained model for future use\n",
    "\n",
    "### Model Performance:\n",
    "The fine-tuned model should now provide more detailed and accurate answers to engineering questions compared to the base GPT-2 model. The training data focused on:\n",
    "- Manufacturing processes (car building, etc.)\n",
    "- Structural engineering (bridges, buildings)\n",
    "- Mechanical systems (engines, motors)\n",
    "- Engineering principles and processes\n",
    "\n",
    "### Next Steps to Improve the Model:\n",
    "1. **Expand Training Data**: Add more diverse engineering Q&A pairs\n",
    "2. **Domain-Specific Training**: Focus on specific engineering domains\n",
    "3. **Longer Training**: Increase epochs for better convergence\n",
    "4. **Larger Model**: Try GPT-2 medium or large (if memory allows)\n",
    "5. **Evaluation Metrics**: Implement proper evaluation metrics\n",
    "6. **Real-World Testing**: Test with actual engineering problems\n",
    "\n",
    "### Memory Optimization Tips for RTX 4060:\n",
    "- Use gradient checkpointing for larger models\n",
    "- Implement dynamic batching\n",
    "- Use DeepSpeed for advanced memory optimization\n",
    "- Consider model quantization techniques\n",
    "\n",
    "### Deployment Options:\n",
    "- Create a web interface using Gradio or Streamlit\n",
    "- Deploy as an API using FastAPI\n",
    "- Integrate into existing engineering tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f39776-fdd5-4bf9-a396-d72110f5cb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c22b8-8d14-4862-949e-21bd4300fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Notebooks (ai-notebooks-env)",
   "language": "python",
   "name": "ai-notebooks-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
